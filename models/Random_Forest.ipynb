{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import KFold, cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🗝️ Cross-validation accuracy scores:[0.72307692 0.76153846 0.71538462 0.74615385 0.6744186 ]\n",
      "🔑 Mean accuracy:0.7241144901610018\n"
     ]
    }
   ],
   "source": [
    "# Read the dataset\n",
    "df = pd.read_csv('/Users/Frankie_C/Documents/GitHub/Academic-Success-Predictor/data/df_vif_cleaned.csv')\n",
    "\n",
    "# Separate features (x) and target (y)\n",
    "x = df.drop('result_pass', axis = 1s)\n",
    "y = df['result_pass']\n",
    "\n",
    "# Define K-fold cross-validation\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Create a RandomForest classifier\n",
    "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf_model.fit(x, y)\n",
    "\n",
    "# Perform Cross-Validation\n",
    "cv_scores = cross_val_score(rf_model, x, y, cv=kf, scoring=\"accuracy\")\n",
    "\n",
    "# Compute mean accuracy\n",
    "mean_accuracy = np.mean(cv_scores)\n",
    "print(f\"🗝️ Cross-validation accuracy scores:{cv_scores}\")\n",
    "print(f\"🔑 Mean accuracy:{mean_accuracy}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NotFittedError",
     "evalue": "This RandomForestClassifier instance is not fitted yet. Call 'fit' with appropriate arguments before using this estimator.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotFittedError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Get feature importance scores\u001b[39;00m\n\u001b[1;32m      2\u001b[0m feature_importance_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame({\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFeature\u001b[39m\u001b[38;5;124m\"\u001b[39m: x\u001b[38;5;241m.\u001b[39mcolumns,\n\u001b[0;32m----> 4\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mImportance\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[43mrf_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeature_importances_\u001b[49m\n\u001b[1;32m      5\u001b[0m })\u001b[38;5;241m.\u001b[39msort_values(by\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mImportance\u001b[39m\u001b[38;5;124m\"\u001b[39m, ascending\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Show important features\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(feature_importance_df)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/ensemble/_forest.py:671\u001b[0m, in \u001b[0;36mBaseForest.feature_importances_\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    650\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[1;32m    651\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfeature_importances_\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    652\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    653\u001b[0m \u001b[38;5;124;03m    The impurity-based feature importances.\u001b[39;00m\n\u001b[1;32m    654\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    669\u001b[0m \u001b[38;5;124;03m        array of zeros.\u001b[39;00m\n\u001b[1;32m    670\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 671\u001b[0m     \u001b[43mcheck_is_fitted\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    673\u001b[0m     all_importances \u001b[38;5;241m=\u001b[39m Parallel(n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_jobs, prefer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthreads\u001b[39m\u001b[38;5;124m\"\u001b[39m)(\n\u001b[1;32m    674\u001b[0m         delayed(\u001b[38;5;28mgetattr\u001b[39m)(tree, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfeature_importances_\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    675\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m tree \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimators_\n\u001b[1;32m    676\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m tree\u001b[38;5;241m.\u001b[39mtree_\u001b[38;5;241m.\u001b[39mnode_count \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    677\u001b[0m     )\n\u001b[1;32m    679\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m all_importances:\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/utils/validation.py:1757\u001b[0m, in \u001b[0;36mcheck_is_fitted\u001b[0;34m(estimator, attributes, msg, all_or_any)\u001b[0m\n\u001b[1;32m   1754\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m   1756\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _is_fitted(estimator, attributes, all_or_any):\n\u001b[0;32m-> 1757\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m NotFittedError(msg \u001b[38;5;241m%\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mtype\u001b[39m(estimator)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m})\n",
      "\u001b[0;31mNotFittedError\u001b[0m: This RandomForestClassifier instance is not fitted yet. Call 'fit' with appropriate arguments before using this estimator."
     ]
    }
   ],
   "source": [
    "# Get feature importance scores\n",
    "feature_importance_df = pd.DataFrame({\n",
    "    \"Feature\": x.columns,\n",
    "    \"Importance\": rf_model.feature_importances_\n",
    "}).sort_values(by=\"Importance\", ascending=False)\n",
    "\n",
    "# Show important features\n",
    "print(feature_importance_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'max_depth': 20, 'max_features': 'sqrt', 'min_samples_leaf': 1, 'min_samples_split': 10, 'n_estimators': 300}\n",
      "Best Cross-Validation Accuracy: 0.7457364341085271\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Read the dataset\n",
    "df = pd.read_csv('/Users/Frankie_C/Documents/GitHub/Academic-Success-Predictor/data/df_vif_cleaned.csv')\n",
    "\n",
    "# Separate features (x) and target (y)\n",
    "x = df.drop(columns=['result_pass'])\n",
    "y = df['result_pass']\n",
    "\n",
    "# Define K-fold cross-validation\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Define the parameter grid for tuning\n",
    "param_grid = {\n",
    "    \"n_estimators\": [100, 200, 300],          # Number of trees\n",
    "    \"max_depth\": [10, 20, None],              # Tree depth limit\n",
    "    \"min_samples_split\": [2, 5, 10],          # Minimum samples to split a node\n",
    "    \"min_samples_leaf\": [1, 2, 4],            # Minimum samples per leaf\n",
    "    \"max_features\": [\"sqrt\", \"log2\"]          # Features considered per split\n",
    "}\n",
    "\n",
    "# Initialize Random Forest model\n",
    "rf_model = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# Perform Grid Search with 5-fold cross-validation\n",
    "grid_search = GridSearchCV(rf_model, param_grid, cv=kf, scoring=\"accuracy\")\n",
    "grid_search.fit(x, y)\n",
    "\n",
    "# Get the best parameters and best score\n",
    "best_params = grid_search.best_params_\n",
    "best_score = grid_search.best_score_\n",
    "\n",
    "print(\"Best Hyperparameters:\", best_params)\n",
    "print(\"Best Cross-Validation Accuracy:\", best_score)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
